---
title: "Atlantis biomass distributions with sdmTMB - lat, lon"
author: "Alberto Rovellini"
date: "3/14/2022"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

This is a template for fitting `sdmTMB` to North Pacific Pelagic Seabird Database data. Starting with a version with lat, lon, and distance from shore, i.e. ignoring the temporal component in the data because of data sparsity. 

The goal of this is obtaining winter distributions for seabirds in the GOA. Colony attendance is reduced in the winter.

This workflow is based on the following assumptions:

1. We use lat, lon and distance from shore as predictors. 
2. We predict over a regular grid. The size of this grid is 10 km at the moment for computational efficiency, but this is arbitrary and we may need to test different grid sizes and see how the results change. This is the grid size we are using for the GOA, but here we are using the same SPDE mesh for a much smaller area, and therefore we might need to adjust the prediction grid accordingly.
3. We are not so interested in accurate predictions for any one year, but rather in representative means of where the birds have been over the last few decades. Here, we run the model without a temporal component. 

```{r}
library(sdmTMB)
library(tidyverse)
library(sf)
library(maps)
library(mapdata)
library(rbgm)
library(viridis)
library(kableExtra)
```

```{r}
select <- dplyr::select
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Read data
```{r}
nppsd_data <- read.csv('../outputs/Surface_fishwinter.csv')

atlantis_bgm <- read_bgm('../data/GOA_WGS84_V4_final.bgm')
atlantis_box <- atlantis_bgm %>% box_sf()
# utilities
atlantis_crs <- atlantis_bgm$extra$projection
atlantis_bbox <- atlantis_box %>% st_bbox()
```

Take a quick look at the data spatially.
```{r, fig.width=12, fig.height=15}
# coast mask
coast <- map("worldHires", regions = c("Canada", "USA"), plot = FALSE, fill = TRUE)
coast_sf <- coast %>% st_as_sf() %>% st_transform(crs = atlantis_crs) %>% st_combine()

ggplot()+
  geom_point(data = nppsd_data, aes(lon*1000, lat*1000, colour = log1p(dens)), size = 1.5)+
  scale_colour_viridis_c()+
  geom_sf(data = coast_sf)+
  coord_sf(xlim=c(atlantis_bbox$xmin, atlantis_bbox$xmax),ylim=c(atlantis_bbox$ymin, atlantis_bbox$ymax))+
  theme_minimal()+
  facet_wrap(~year, ncol = 4)+
  labs(title = paste(nppsd_data$Proposed.group,"density from NPPSD", sep = " "))
```
It goes way back and it is pretty sparse. The sampling methods may also differ between observations.

Take a quick look at time series of total density from survey data. 
```{r, fig.align="center"}
dens_year <- nppsd_data %>% group_by(year) %>% summarise(dens = sum(log1p(dens), na.rm = TRUE))

ggplot(dens_year, aes(year, log(dens)))+
  geom_point()+
  geom_path()+
  theme_minimal()+
  labs(title = paste(nppsd_data$name,"total GOA density from NPPSD"))
```

# sdmTMB

## Create spatial mesh

Read in the Atlantis BGM, then turn the bottom trawl data to an sf object, reproject it, and then turn it back to coordinates.
```{r}
atlantis_bgm <- read_bgm('../data/GOA_WGS84_V4_final.bgm')
atlantis_crs <- atlantis_bgm$extra$projection

nppsd_data_sf <- nppsd_data %>% 
  mutate(lat=lat*1000, lon=lon*1000) %>%
  st_as_sf(coords = c("lon", "lat"))
```

Adding an `sf` object of the coastline to incorporate in the mesh.
```{r}
data_bbox <- nppsd_data_sf %>% st_bbox()
coast_mesh <- coast %>% 
  st_as_sf() %>% 
  st_transform(crs = atlantis_crs) %>% 
  st_transform(crs=atlantis_bgm$extra$projection) %>%
  st_crop(data_bbox)
```

Using the "cutoff" argument, instead of predefining the number of points. This will help with AFSC vs DFO (use the same distance), also it does not depend on the random seed or on the order of the data.

**Note:** SPDE = Stochastic Partial Differential Equations approach. Some material can be found [here](https://becarioprecario.bitbucket.io/spde-gitbook/ch-intro.html#sec:spde), but basically it is a way of calculating the position of the mesh knots. 

We take land barriers into account when building the mesh (see `sdmTMB` documentation).
```{r}
nppsd_spde <- make_mesh(nppsd_data, c("lon", "lat"), cutoff = 20, type = "cutoff")

# add barrier
nppsd_spde <- add_barrier_mesh(
  spde_obj = nppsd_spde,
  barrier_sf = coast_mesh,
  range_fraction = 0.1,
  proj_scaling = 1000, # data km but projection m
  plot = TRUE
)

nppsd_spde$mesh$n
```

Check out the distribution of the biomass density response variable.
```{r, fig.width = 6, fig.height = 4}
hist(nppsd_data$dens, breaks = 30)
```

```{r, fig.width = 6, fig.height = 4}
hist(log1p(nppsd_data$dens), breaks = 30)
```

Proportion of zeroes in percentage.
```{r}
length(which(nppsd_data$dens == 0))/nrow(nppsd_data)*100
```

## Space, time, and distance from coast model.

Model type: the distribution of the response variable plotted above should give a sense of what model is most appropriate. CPUE data for many of these species resemble a Tweedie distribution when log-transformed, so we use a Tweedie model with a log link. Some groups may warrant a different model, and this will be evaluated case-by-case depending on convergence issues, distribution of model residuals, and model skill metrics (see below).

Distance from shore is in km here. We use a spline with 3 knots like in the depth models to start. Not using year effects (data is too sparse).
```{r, results = FALSE}
start.time <- Sys.time()

m_dist <- sdmTMB(
    data = nppsd_data, 
    formula = dens ~ 0 + s(distance, k = 3),# + as.factor(year), 
    mesh = nppsd_spde, 
    time = NULL, 
    spatial = 'on',
    spatiotemporal = 'off', # spatiotemporal random fields independent and identically distributed
    reml = TRUE,
    anisotropy = FALSE,
    silent = FALSE,
    family = tweedie(link = "log"))

end.time <- Sys.time()
time.taken_m_depth <- end.time - start.time
time.taken_m_depth
```

Rerun with extra optimization steps in case of gradient > 0.001.
```{r, results = FALSE}
if(abs(max(m_dist$gradients))>0.001){
  
  m_dist <- sdmTMB(
    data = nppsd_data, 
    formula = dens ~ 0 + s(distance, k = 3),# + as.factor(year), 
    mesh = nppsd_spde, 
    time = NULL, 
    spatial = 'on',
    spatiotemporal = 'off', # spatiotemporal random fields independent and identically distributed
    reml = TRUE,
    anisotropy = FALSE,
    silent = FALSE,
    control = sdmTMBcontrol(nlminb_loops = 1, newton_steps = 3),
    family = tweedie(link = "log"))
  
  end.time <- Sys.time()
  time.taken_m_depth <- end.time - start.time
  time.taken_m_depth
}
```

Check information on model convergence. From [the nlminb help page](https://rdrr.io/r/stats/nlminb.html) we know that an integer 0 indicates succesful convergence. Additional information on convergence can be checked with `m_depth\$model\$message`. According to the original [PORT optimization documentation](https://web.archive.org/web/20070203144320/http://netlib.bell-labs.com/cm/cs/cstr/153.pdf), "Desirable return codes are 3, 4, 5, and sometimes 6".  
```{r}
if(m_dist$model$convergence == 0){print("The model converged.")} else {print("Check convergence issue.")}
m_dist$model$message # convergence message
max(m_dist$gradients) # maximum gradient component 
tidy(m_dist, effects = 'ran_pars') %>% filter(term=='range') %>% pull(estimate) # Matérn range
```

Check out model residuals.
```{r, fig.width = 6, fig.height = 4}
nppsd_data$resids <- residuals(m_dist) # randomized quantile residuals
hist(nppsd_data$resids)
```

And QQ plot.
```{r}
qqnorm(nppsd_data$resids)
abline(a = 0, b = 1)
```
Heavy skew for 'Diving_Fish', for example. Bad fit for high densities?

Plot the response curve from the depth smooth term.
```{r}
plot_smooth(m_dist, ggplot = T)
```

Finally, plot the residuals in space. If residuals are constantly larger/smaller in some of the areas, it may be sign that the model is biased and it over/underpredicts consistently for some areas. Residuals should be randomly distributed in space. 

```{r, fig.width = 12, fig.height=6}
nppsd_sf <- nppsd_data %>% mutate(lon=lon*1000,lat=lat*1000) %>% st_as_sf(coords = c(x = "lon", y = "lat"), crs = atlantis_crs) # turn to spatial object

#define coordinate limits for BGM projection
coord_lims <- nppsd_sf %>% st_coordinates() %>% data.frame() %>% set_names(c("x","y")) %>% summarise(xmin=min(x),xmax=max(x),ymin=min(y),ymax=max(y))

ggplot()+
  geom_sf(data = nppsd_sf, aes(color = resids, alpha = .8))+
  scale_color_viridis()+
  geom_sf(data = coast_sf)+
  coord_sf(xlim = c(coord_lims$xmin,coord_lims$xmax), ylim=c(coord_lims$ymin,coord_lims$ymax))+
  theme_minimal()+
  labs(title = paste(nppsd_data$name,"model residuals in space", sep = " "))#+
  #facet_wrap(~year, ncol = 2)
```

# Predictions from SDM

Take a grid (which must contain information on the predictors we used to build the model) and predict the biomass index over such grid based on the predictors. The grid is currently a regular grid with 10-km cell size, but 10 km might not be enough to get prediction points in all boxes - especially for a couple very small and narrow boxes at the western end of the model domain. Revisit this if necessary, but a finer mesh could be difficult to justify compared to the density of the survey data. The grid covers the entire Atlantis model domain, including the non-dynamic boundary boxes (deeper than 1000 m).

Read in the Atlantis prediction grid (10 km) modified in `Atlantis_grid_covars.R` (code not included here).

**For NPPSD:** This step will be different:

- Read Atlantis grid.
- Ditch depth.
- Turn it to sf.
- Calculate each point's distance from shore.
- Divide coords by 1000.
- Use dist and linear coords as predictors.

```{r}
load("../data/atlantis_grid_depth.Rdata")

atlantis_grid <- atlantis_grid_depth %>% 
  select(-depth,-insideY,-insideX) %>% 
  st_as_sf(coords=c("x","y"), crs = atlantis_crs) %>%
  mutate(distance = as.vector(st_distance(geometry,coast_sf))/1000) %>%
  mutate(lon=st_coordinates(geometry)[,1]/1000,lat=st_coordinates(geometry)[,2]/1000) %>%
  st_set_geometry(NULL)
```

```{r}
# add year column
# all_years <- levels(factor(goaierp_data$year))
# 
# atlantis_grid <- atlantis_grid_dist[rep(1:nrow(atlantis_grid_dist), length(all_years)),]
# atlantis_grid$year <- as.numeric(rep(all_years, each = nrow(atlantis_grid_depth)))
```

Make SDM predictions onto new data from depth model. **Back-transforming here, is this sensible?**
```{r}
predictions_nppsd <- predict(m_dist, newdata = atlantis_grid, return_tmb_object = TRUE)
atlantis_grid$estimates <- exp(predictions_nppsd$data$est) #Back-transforming here, is this sensible?

atlantis_grid_sf <- atlantis_grid %>% mutate(lon=lon*1000,lat=lat*1000) %>% st_as_sf(coords = c("lon", "lat"), crs = atlantis_crs) # better for plots
coord_lims <- atlantis_grid_sf %>% st_bbox()
```

Plotting Canada as well here, only because if we leave that out we need to leave out the AI as well. It will be best to replace Canada predictions with Canada data.
```{r, fig.width = 12, fig.height = 5.5}
ggplot()+
  geom_sf(data = atlantis_grid_sf, aes(color=log1p(estimates)), size = 2)+ # taking the log for visualisation
  geom_sf(data = coast_sf)+
  coord_sf(xlim = c(coord_lims$xmin,coord_lims$xmax), ylim=c(coord_lims$ymin,coord_lims$ymax))+
  scale_color_viridis(name = expression(paste("Log-density num ", km^-2)))+
  theme_minimal()+
  labs(title = paste(nppsd_data$name,"predicted density", sep = " "))
```

Attribute the predictions to their respective Atlantis box, so that we can take box averages.
```{r}
atlantis_grid_means <- atlantis_grid %>% group_by(box_id) %>%
  summarise(mean_estimates = mean(estimates, na.rm = TRUE)) %>% ungroup() 

# join this with the box_sf file

predictions_by_box <- atlantis_box %>% inner_join(atlantis_grid_means, by = "box_id")
```

See estimates per box for all years combined. Silence boundary boxes as they throw the scale out of whack (and they do not need predictions). 
```{r, fig.width = 12, fig.height = 5.5}
predictions_by_box <- predictions_by_box %>% rowwise() %>% mutate(mean_estimates = ifelse(isTRUE(boundary), NA, mean_estimates))

ggplot()+
  geom_sf(data = predictions_by_box, aes(fill = log1p(mean_estimates)))+ # taking the log for visualisation
  scale_fill_viridis(name = expression(paste("Log-density num ", km^-2)))+
  theme_minimal()+
  geom_sf(data = coast_sf)+
  coord_sf(xlim = c(coord_lims$xmin,coord_lims$xmax), ylim=c(coord_lims$ymin,coord_lims$ymax))+  
  labs(title = paste(nppsd_data$name, "mean predicted density by Atlantis box", sep = " "))
```
We may consider not using 2010 if we see that it throws the model out of whack, because it was much fewer data points compared to other years.

Plot the raw data again for comparison.
```{r, fig.width = 12, fig.height = 5.5}
nppsd_data %>% mutate(lon=lon*1000,lat=lat*1000) %>% st_as_sf(coords = c("lon","lat"), crs=atlantis_crs) %>% 
  filter(dens>0) %>% # just for visualisation
  ggplot()+
  geom_sf(aes(colour = log1p(dens)), size = 2, alpha = .5)+ # taking the log for visualisation
  scale_colour_viridis_c(name = expression(paste("Log-density num ", km^-2)))+
  geom_sf(data = coast_sf)+
  coord_sf(xlim = c(min(nppsd_data$lon*1000),max(nppsd_data$lon*1000)), ylim=c(min(nppsd_data$lat*1000),max(nppsd_data$lat*1000)))+  theme_minimal()+
  #facet_wrap(~year, ncol = 2)+
  labs(title = paste(nppsd_data$name,"Density from NPPSD", sep = " "))
```

Have a look at density by distance from shore. 
```{r, fig.width = 6, fig.height = 4}
ggplot(data = nppsd_data, aes(x = distance, y = log1p(dens)))+#, color = log1p(num_km2)))+
  scale_color_viridis()+
  geom_point()+
  theme_minimal()+
  labs(title = "Density and distance from shore")
```

Plot data and predictions distributions. These are the data.
```{r, fig.width = 6, fig.height = 4}
ggplot(data = nppsd_data, aes(log1p(dens)))+
  geom_histogram(colour = "black", fill = 'grey80')+
  theme_minimal()
```

And these are the predictions over the 10 km grid.
```{r, fig.width = 6, fig.height = 4}
ggplot(data = atlantis_grid, aes(log1p(estimates)))+
  geom_histogram(colour = "black", fill = 'grey80')+
  theme_minimal()
```

Let’s have a look at the variance per box over all years. We use the coefficient of variation, because CPUE varies widely between boxes.

```{r}
atlantis_grid_cv <- atlantis_grid %>% group_by(box_id) %>% summarise(cv = sd(estimates)/mean(estimates)) %>% ungroup()

cv_by_box <- atlantis_box %>% inner_join(atlantis_grid_cv, by = "box_id")
```

```{r}
ggplot()+
  geom_sf(data = cv_by_box[-which(cv_by_box$boundary),], aes(fill = cv))+
  scale_fill_viridis(name = "CV of density")+
  geom_sf(data = coast_sf, colour = "black", fill = "grey80")+
  theme_minimal()+
  coord_sf(xlim = c(coord_lims$xmin,coord_lims$xmax), ylim=c(coord_lims$ymin,coord_lims$ymax))+
  labs(title = paste(nppsd_data$name, "CV of predicted density by Atlantis box", sep = " "))
```

# Model skill

Trying to evaluate model skill by having a look at how well model predictions align with observations.

Since this is a spatially-explicit approach, we need observations and predictions at the same location. We use the locations of all NPPSD as a prediction grid.
```{r}
#make a prediction grid from the nppsd data itself
nppsd_grid <- nppsd_data %>% dplyr::select(lon, lat, distance) 

# predict on this grid
predictions_at_locations <- predict(m_dist, newdata = nppsd_grid, return_tmb_object = TRUE)
nppsd_grid$predictions <- exp(predictions_at_locations$data$est) # back-transforming here
```

Now join by year and coordinates to have predictions at the sampling points.
```{r, fig.width = 12, fig.height = 6}
nppsd_corr <- nppsd_data %>% mutate(pred_at_obs=nppsd_grid$predictions)
```

## Observed versus predicted

```{r}
paste0("Pearson's coef observations vs predictions: ", cor(nppsd_corr$dens, nppsd_corr$pred_at_obs, use = "everything", method = "pearson"))
```
What is a good value here?

Plot.
```{r}
ggplot(nppsd_corr, aes(x = log1p(dens), y = log1p(pred_at_obs)))+ # log for visualisation
  geom_point(aes(color = distance))+
  scale_color_viridis()+
  geom_abline(intercept = 0, slope = 1)+
  theme_minimal()+
  # facet_wrap(~year, scales = "free")+
  labs(title = paste(nppsd_data$name, "observed vs predicted density", sep = " "))
```
Correlations is really low, which means the model is doing a poor job.

Plot zero catch from the data and the relative predictions. Turn to sf for plotting.

```{r}
nppsd_corr %>% filter(dens == 0) %>%
  mutate(lon=lon*1000,lat=lat*1000) %>%
  st_as_sf(coords = c(x = "lon", y = "lat"), crs = atlantis_crs) %>%
  ggplot()+
  geom_sf(aes(color = log1p(pred_at_obs)))+
  geom_sf(data = coast_sf)+
  coord_sf(xlim = c(min(nppsd_data$lon*1000),max(nppsd_data$lon*1000)), ylim=c(min(nppsd_data$lat*1000),max(nppsd_data$lat*1000)))+  theme_minimal()+
  scale_color_viridis()+
  theme_minimal()+
  labs(title = "Model predictions at zero-density locations")#+
  #facet_wrap(~year, ncol = 2)
```

What about the relationship between model residuals and distance from shore?
```{r}
nppsd_data %>%
  ggplot()+
  geom_point(aes(x = distance, y = resids, color = log1p(dens)))+
  geom_hline(yintercept = 0, color = "red", linetype = "dashed")+
  scale_color_viridis()+
  theme_minimal()
```

## Root Mean Square Error (RMSE)

Calculate RMSE between predicted and observed values.
```{r}
paste("RMSE:", sqrt(sum((nppsd_corr$pred_at_obs - nppsd_corr$dens)^2)/nrow(nppsd_corr)), " num km-2", sep = " ") ### traditional rmse metric, in units kg km2
```

Normalized RMSE. 
```{r}
rmse_cv <- sqrt(sum((nppsd_corr$pred_at_obs - nppsd_corr$dens)^2)/nrow(nppsd_corr))/(max(nppsd_corr$dens)-min(nppsd_corr$dens))*100 #### normalised rmse, expressed as a % of the range of observed biomass values, sort of approximates a coefficient of variation 
paste("Normalised RMSE:", paste0(rmse_cv, "%"), sep = " ")
```
What is a good value here?

# Total biomass and biomass per box

The current estimated CPUE is in kg km$^{-2}$. So, just I just turn that into biomss per box. Remember that the area is in m$^2$ for the boxes, so need to divide by 1,000,000.
```{r}
predictions_by_box <- predictions_by_box %>% mutate(abundance = mean_estimates*area*1e-06) %>% ungroup()

predictions_by_box %>% select(box_id, mean_estimates, abundance) %>% st_set_geometry(NULL) %>% kable(align = 'lccc', format = "markdown", 
      col.names = c("Box", "Density (num km-2)", "Abundance"))
```

Fill boxes with zero abundance with the smallest proportion in the model, and take those out of the highest proportion.
```{r}
out <- predictions_by_box %>% 
  st_set_geometry(NULL) %>% 
  select(.bx0,botz,boundary,abundance) %>%
  full_join(atlantis_box %>% select(.bx0,botz,boundary) %>% st_set_geometry(NULL), by=c('.bx0','botz','boundary')) %>%
  mutate(Prop=abundance/sum(abundance,na.rm=TRUE))

out$Prop[is.na(out$Prop)]<-0 # turn NAs to 0's

min_prop <- out %>% filter(Prop>0 & boundary==F & botz<0) %>% pull(Prop) %>% min()
max_prop <- out %>% filter(Prop>0 & boundary==F & botz<0) %>% pull(Prop) %>% max()

# how many boxes are empty?
box_no_bird <- out %>% filter(Prop==0 & boundary==F & botz<0) %>% nrow()

out$Prop[out$Prop==0 & out$boundary==F & out$botz<0] <- min_prop
out$Prop[out$Prop==max_prop] <- out$Prop[out$Prop==max_prop] - (min_prop*box_no_bird)

out <- out %>% arrange(.bx0)
```

Write out a .csv.
```{r}
write.csv(x = out, 
          file = paste0("../winter_dists/",nppsd_data$name[1],"_winter_NPPSD.csv"), 
          row.names = FALSE)
```

# Validation metrics

Let’s produce a table that includes: convergence metrics; Pearson’s correlation coefficient for predicted vs observed; RMSE; and normalised RMSE.
```{r}
val <- data.frame(nppsd_data$name[1], # group
                  m_dist$model$convergence, # convergence
                  m_dist$model$message, # more convergence
                  max(m_dist$gradients), # max gradient
                  tidy(m_dist, effects = 'ran_pars') %>% filter(term=='range') %>% pull(estimate), # Matérn range
                  cor(nppsd_corr$dens, nppsd_corr$pred_at_obs, use = "everything", method = "pearson"), # correlation
                  sqrt(sum((nppsd_corr$pred_at_obs - nppsd_corr$dens)^2)/nrow(nppsd_corr)),# RMSE
                  sqrt(sum((nppsd_corr$pred_at_obs - nppsd_corr$dens)^2)/nrow(nppsd_corr))/(max(nppsd_corr$dens)-min(nppsd_corr$dens))*100 # NRMSE
) %>% set_names(c("Group","Convergence","Message","Max gradient","Practical range (km)","Pearson's correlation","RMSE","NRMSE(%)"))

val
```

```{r}
write.csv(x = val, 
          file = paste0("../winter_dists/","validation_",nppsd_data$name[1],"_winter_NPPSD.csv"), 
          row.names = FALSE)
```
